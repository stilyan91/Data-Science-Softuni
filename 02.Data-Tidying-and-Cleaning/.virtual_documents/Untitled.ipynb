%matplotlib inline


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt







pew = pd.read_csv('data/pew.csv')


pew


pew_tidy = pew.melt(
    id_vars=["religion"], # Identifier variables (all other are "unpivoted"
    var_name= "income", # Variable
value_name = "frequency") #value


pew_tidy


pew_tidy[pew_tidy.frequency == 0]


tb = pd.read_csv('data/tb.csv') # tb = pd.read_csv('data/tb.csv', na_values = ["?",-999, "NA"]


tb.head


tb.shape


# Check if any value in iso2 column is NaN
tb.iso2.isna().any()


# Visualize the rows with NaN as value for column iso2
tb[tb.iso2.isna()]


# using loc[index, column] we take every row in iso2 column that value is NaN to become "NA"
tb.loc[tb.iso2.isna(), "iso2"] = "NA"


# Again check if we still have NaN value in iso2 column
tb[tb.iso2.isna()]


# check types of the columns
tb.dtypes


# Because count is not equal in every row that means there is a lot of missing records
tb.describe().T


tb.head()


# check names of columns
tb.columns


# we melt data to keep iso2 and year as attributes and makes every other column as measured variables 
tb_tidy = tb.melt(id_vars=["iso2", "year"],var_name="sex_and_age",value_name="cases")


tb_tidy


# delete all rows where there is NaN as valueabs 
tb_tidy = tb_tidy.dropna()
# 115380 rows


tb_tidy
# 35750 rows 


# slice is like slicing in python
tb_tidy.sex_and_age.str.slice(0,1) # == tb_tidy.sex_and_age.str.get(0)


# we can put it like new columns
tb_tidy["sex"] = tb_tidy.sex_and_age.str.get(0)
tb_tidy["age_group"] = tb_tidy.sex_and_age.str.slice(1)


tb_tidy


# That means old column sex_and_age is unnecessary and we can drop it
tb_tidy = tb_tidy.drop(columns=["sex_and_age"])


tb_tidy


# making function to change some denotes in table
def process_age_group(age_group):
    ages = {"04": "0-4", "65": "65+", "u": "unknown"}
    if age_group in ages:
        return ages[age_group]
    else:
        return f"{age_group[:-2]}--{age_group[-2:]}"


# use function to change values in column age_group
tb_tidy.age_group = tb_tidy.age_group.apply(process_age_group)


# tidy up the column and row order 
tb_tidy = tb_tidy[["iso2", "year", "sex", "age_group", "cases"]]
tb_tidy = tb_tidy.sort_values(["iso2", "year"])


tb_tidy


weather_data = pd.read_csv('data/weather.csv')


weather_data


# check all values for this elemnt column
weather_data.element.unique()


#melt columns to transform day from column to row
weather_data = weather_data.melt(id_vars=["id", "year", "month", "element"], var_name="day", value_name="value")


# check day unique values
weather_data.day.unique()


# transform day column from string to int 
weather_data.day = weather_data.day.str.slice(1).astype(int)


# tmin and tmax we want to make it as columns
weather_data = weather_data.pivot_table(index=["id", "year", "month", "day" ], columns=["element"], values="value")


weather_data = weather_data.reset_index()


# reorder columns with list 
weather_data[["id","tmin", "tmax", "day", "month", "year"]]


weather_data.day


weather_data["day"] # it returns pandas.Series
type(weather_data["day"]) # 1 column


weather_data[["day", "month"]] # it returns pandas.DataFrame
type(weather_data[["day"]]) # table with 1 column


weather_data["date"] = pd.to_datetime(weather_data[["year", "month", "day"]])


weather_data = weather_data.drop(columns=["year", "month", "day"])


weather_data = weather_data[["date", "tmin", "tmax"]]


weather_data.to_csv('data/weather_tidy.csv', index=None)





billboard = pd.read_csv('data/billboard.csv')


billboard[billboard.wk65.notna()]


billboard


# rename a column
billboard = billboard.rename(columns={"date.entered": "date_entered"}) 


billboard_tidy = billboard.melt(
    id_vars=["year", "artist", "track", "time", "date_entered"],
    var_name="week", 
    value_name="position" )


# check if every one value of column week start with "wk"
billboard_tidy.week.str.startswith("wk").all()


# transform it into int 
billboard_tidy.week = billboard_tidy.week.str.slice(2).astype(int)


billboard_tidy.dtypes


billboard_tidy


# drop all NaN  
billboard_tidy = billboard_tidy.dropna()


billboard_tidy


# every songs has attribute year=2000 that means we can remove this column
billboard_tidy.year.unique()


# check songs that are in 10 week
billboard_tidy[billboard_tidy.week==10]


# we take whole history of specific song
billboard_tidy[(billboard_tidy.artist=="3 Doors Down") & (billboard_tidy.track=="Kryptonite")]


# we can also plot it 
plt.plot(billboard_tidy[(billboard_tidy.artist=="3 Doors Down") & (billboard_tidy.track=="Kryptonite")].position)


# we can separate dataset into  2 divided dataset: either is for songs, either is for positions
songs = billboard_tidy[["track", "artist", "time"]].drop_duplicates(ignore_index=True) #==.reset_index(drop=True)


songs


# songs['id'] = songs.index + 1


# songs=songs[['track', 'artist', 'time']]


# making second table
song_info = billboard_tidy[["date_entered", "week", "position"]].drop_duplicates().reset_index(drop=True)


song_info


songs['key'] = songs.index
song_info['key'] = song_info.index


# merge the tables
merged_songs = pd.merge(songs, song_info, on=['key'], how='inner')





weather_data[
        (weather_data.tmin >= 15) & 
        (weather_data.tmin < 17) |
        (weather_data.date.dt.month==8)
]


weather_data[~(weather_data.date.dt.month == 8)]


(weather_data[~(weather_data.date.dt.month == 8)].date.dt.month==8).all()


weather_data[["date","tmin"]]


billboard_tidy[["artist", "track"]]


# projection 
billboard['artist_short_name'] = billboard.artist.str.slice(0,5) + "..."


billboard


billboard_tidy.describe().T


weather_data.tmin.min()


weather_data.mean()


pew_tidy.groupby("religion").frequency.mean().sort_values(ascending=True)


pew_tidy.groupby("religion").frequency.mean()


plt.barh(pew_tidy.groupby("religion").frequency.mean().sort_values().index,pew_tidy.groupby("religion").frequency.mean().sort_values())
plt.title("Mean number of people by religion")
plt.xlabel("mean number of people")
plt.show()


aggregates = {"religion": [], 
             "min": [],
             "max": [],
             "mean": []}
for (religion, group_date) in pew_tidy.groupby("religion"):
    print(religion, group_date.frequency.min(), group_date.frequency.mean(), group_date.frequency.max(), sep="; ")
    aggregates['religion'].append(religion)
    aggregates['min'].append(group_date.frequency.min())
    aggregates['mean'].append(group_date.frequency.mean())
    aggregates['max'].append(group_date.frequency.max())


pd.DataFrame(aggregates)


weather_data[~(weather_data.tmin > 15)] 





us_weather_data = pd.read_csv('https://raw.githubusercontent.com/synesthesiam/blog/master/posts/data/weather_year.csv')


#check shape
us_weather_data.shape


#check types of columnns 
us_weather_data.dtypes
# notice that some names have space as prefix 
# some separators are ',' others are ", "


# rename a columns names where there is a space in names
us_weather_data = us_weather_data.rename(columns = lambda x: x.strip())


us_weather_data.dtypes


# basic rename
us_weather_data.columns = [
    "date", "max_temp", "mean_temp", "min_temp", "max_dew",
    "mean_dew", "min_dew", "max_humidity", "mean_humidity",
    "min_humidity", "max_pressure", "mean_pressure",
    "min_pressure", "max_visibility", "mean_visibility",
    "min_visibility", "max_wind", "mean_wind", "max_gusts",
    "precipitation", "cloud_cover", "events", "wind_dir"
    ]


us_weather_data.describe().T


# dtypes shows us that precipitation is object, that means there is some string as value. We need to fix it. 
# check values in this column
us_weather_data.precipitation.unique()
# and  we see that there is 'T' (trace value - very small value)


# we change trace values with very small number 
us_weather_data.loc[us_weather_data.precipitation == "T" , "precipitation"] = 1e-15


# convert precipitation as float
us_weather_data.precipitation = us_weather_data.precipitation.astype(float)


# convery date column to datetime type
us_weather_data.date = pd.to_datetime(us_weather_data.date)


# set date as index 
us_weather_data = us_weather_data.set_index('date')


us_weather_data.loc["2013/03/06"]


us_weather_data[us_weather_data.max_temp > 80]


#take columns where "temp" is in there names
us_weather_data.columns[us_weather_data.columns.str.contains('temp')]


us_weather_data[
us_weather_data.columns[
    (us_weather_data.columns.str.contains('temp')) | 
    (us_weather_data.columns.str.contains('dew'))
]]



us_weather_data.events.count()


us_weather_data.events.unique()


us_weather_data.cloud_cover.unique()


# some grouping
for (cloud_cover, events), group_data in us_weather_data.groupby(["cloud_cover", "events"]):
    print(cloud_cover, events, len(group_data))


 
