get_ipython().run_line_magic("matplotlib", " inline")








import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression








# Generate data.
num_points = 2000
x = np.linspace(-3, 5, num_points)
y = 2 * x + 3
y_noise = np.random.normal(loc=0, scale=2, size=num_points)
y += y_noise
# the noise must be not dependent from the data


plt.scatter(x, y, s=5)
plt.xlabel('x (input)')
plt.ylabel('y (output)')

plt.show()


data = pd.DataFrame({'x':x, 'y':y})
data = data.sample(len(data))
data.to_csv('data.csv', index=None)


data = pd.read_csv('data.csv')


data


# y = a * x + b
# we are searching the best value for a and b
x = data.x.values
y = data.y.values


a,b = 5, 3


def mse(observer, estimated):
    return ((y - estimated) ** 2).mean()


def plot_model(x,y, a,b):
    y_model = a * x + b
    model_mse = mse(y, y_model)
    plt.plot(x, y_model, c='r', label=f'y= {a:.2f}x+{b:.2f}; mse={model_mse:.2f}')
    plt.scatter(x, y)

    plt.xlabel('x')
    plt.ylabel('y (output)')
    plt.legend()
    plt.show()


plot_model(x,y,5,0)


plot_model(x,y,-3,2)


plot_model(x,y,5,3)


plot_model(x,y,1,1)


n_trails = 100
random_a = np.random.uniform(-5,5, n_trails)
random_b = np.random.uniform(-5, 5, n_trails)


min_mse, best_a, best_b = 1e50, 0, 0
for current_a in random_a:
    for current_b in random_b:
        y_model = current_a * x + current_b
        current_mse = mse(y, y_model)
        if current_mse < min_mse:
            min_mse = current_mse
            best_a = current_a
            best_b = current_b

min_mse


best_a, best_b


plot_model(x,y, best_a, best_b)


a,b = 5, 18
alpha = 0.001
a_gradient = -2 / len(x) * np.sum(x * (y - (a * x +b)))
b_gradient = -2 / len(y) * np.sum(y-(a*x+b))


y_tilde = a * x + b
mse(y, y_tilde)


[a_gradient, b_gradient]


new_a = a - alpha*a_gradient


new_b = b - alpha * b_gradient


y_tilde = new_a * x + new_b
mse(y, y_tilde)


new_b = new_b - alpha * b_gradient
new_a = new_a - alpha*a_gradient
y_tilde = new_a * x + new_b
mse(y, y_tilde)


for i in range(100):
    new_b = new_b - alpha * b_gradient
    new_a = new_a - alpha*a_gradient
y_tilde = new_a * x + new_b
mse(y, y_tilde)


def perform_gradient_descent(x, y, a, b, learning_rate):
    a_gradient = -2 / len(x) * np.sum(x * (y - (a * x +b)))
    b_gradient = -2 / len(y) * np.sum(y-(a*x+b))
    new_a = a - a_gradient * learning_rate
    new_b = b - b_gradient * learning_rate
    return (new_a, new_b)


model_a, model_b = -10, 20 # Start point; can be anywhere
alpha = 0.01 # Learning rate
all_errors = []
for step in range(1001):
    model_a, model_b = perform_gradient_descent(x, y, model_a, model_b, alpha)
    if step % 100 == 0:
        y_model = model_a * x + model_b
        error = mse(y, y_model)
        all_errors.append(error)
        print("step {}: a = {}, b = {}, J = {}".format(step, model_a, model_b, error))
print("Final line: {} * x + {}".format(model_a, model_b))


plt.plot(all_errors)


model_a, model_b = -10, 20 # Start point; can be anywhere
alpha = 0.01 # Learning rate
all_errors = []
for step in range(1001):
    model_a, model_b = perform_gradient_descent(x, y, model_a, model_b, alpha)
    if step % 100 == 0:
        y_model = model_a * x + model_b
        error = mse(y, y_model)
        all_errors.append(error)
print("Final line: {} * x + {}".format(model_a, model_b))


plt.plot(all_errors)
plt.xlabel("Epoch")
plt.ylabel("Training loss")


model_a, model_b


plot_model(x,y, model_a, model_b)



